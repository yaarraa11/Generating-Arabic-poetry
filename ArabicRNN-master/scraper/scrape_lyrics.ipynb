{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_singers(url=None):\n",
    "    if url is None:\n",
    "        raise ValueError('You have to enter a valid URL')\n",
    "    else: \n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        table = soup.find_all(\"div\", class_=\"itemListSubCategories\")[0]\n",
    "        singers = table.find_all('a')\n",
    "        all_singers = []\n",
    "        for singer in singers:\n",
    "            all_singers.append([singer.text.strip(), singer.get('href')])\n",
    "        all_singers_df = pd.DataFrame(all_singers, columns=['name', 'link'])\n",
    "    return all_singers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lyrics(song_url=None):\n",
    "    if song_url is None:\n",
    "        raise ValueError('You have to enter a valid URL')\n",
    "    else: \n",
    "        base = 'http://fnanen.net'\n",
    "        url = ''.join(base + song_url)\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        lyrics = soup.find_all(\"div\", class_=\"itemFullText\")[0]\n",
    "        raw_lyrics = re.sub('<[^>]*>', '\\n', str(lyrics))    \n",
    "    return raw_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_songs_from_url(url=None, with_lyrics = True, verbose=True):\n",
    "    if url is None:\n",
    "        raise ValueError('You have to enter a valid URL')\n",
    "    else:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        table = soup.find_all(\"div\", class_=\"itemListView\")[0]\n",
    "        titles = table.find_all('a',attrs={'class':'', 'data-animation':'true'})    \n",
    "        songs = []\n",
    "        if with_lyrics:\n",
    "            for title in titles:\n",
    "                songs.append([title.text, title.get('href'), get_lyrics(title.get('href'))])     \n",
    "                all_songs_df = pd.DataFrame(songs, columns=['title', 'link', 'lyrics'])\n",
    "                if verbose: \n",
    "                    print('Song ', title.text, ' fetched.')\n",
    "        else:\n",
    "            for title in titles:\n",
    "                songs.append([title.text, title.get('href')])\n",
    "                all_songs_df = pd.DataFrame(songs, columns=['title', 'link'])\n",
    "                if verbose: \n",
    "                    print('Song ', title.text, ' fetched.')\n",
    "    return all_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_songs(url=None, with_lyrics=True, verbose=True):\n",
    "    base = 'http://fnanen.net'\n",
    "    if url is None:\n",
    "        raise ValueError('You have to enter a valid URL')\n",
    "    else:\n",
    "        r = requests.get(''.join(base + url))\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        table = soup.find_all(\"div\", class_=\"itemListView\")[0]\n",
    "        titles = table.find_all('a',attrs={'class':''})        \n",
    "        pagination = soup.find_all('div', 'k2Pagination')        \n",
    "        songs_df = pd.DataFrame({'title':[], 'link':[],'lyrics':[]})\n",
    "        links = [''.join(base + url)]\n",
    "        \n",
    "        if pagination != []:\n",
    "            pages = pagination[0].find_all('a', attrs={'title':['2','3','4']})\n",
    "            for p in pages:\n",
    "                links.append(''.join(base + p.get('href')))\n",
    "        \n",
    "        for link in links:\n",
    "            if verbose: \n",
    "                print('Parsing.. ', link)\n",
    "            try:\n",
    "                songs = get_songs_from_url(link, with_lyrics=with_lyrics, verbose=verbose)\n",
    "                songs_df = songs_df.append(songs, ignore_index=True)\n",
    "            except:\n",
    "                print('ERROR: broken link? check ', link)\n",
    "                continue    \n",
    "                \n",
    "    return songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://fnanen.net/')\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "letters = soup.find_all(\"ul\", class_=\"menu menu-vertical dropdown-hover \")[0]\n",
    "subpages = [''.join('http://fnanen.net' + l.get('href')) for l in letters.find_all('a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subpages.remove(subpages[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_songs = []\n",
    "for page in subpages[17:]:\n",
    "    m_singers = get_singers(page)\n",
    "    for singer in m_singers.iterrows():\n",
    "        url = singer[1][1]\n",
    "        songs = get_songs(url, with_lyrics=True, verbose=False)\n",
    "        songs = songs.assign(singer=singer[1][0])\n",
    "        all_songs.append(songs)\n",
    "        print('done with ', singer[1][0], '. All songs len is: ', len(all_songs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_songs = pd.concat(all_songs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_songs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_songs.to_pickle('all_songs.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
